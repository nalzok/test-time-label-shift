{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n",
    "# https://github.com/tensorflow/probability/issues/1523\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 04:21:30.355836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 04:21:30.392375: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 04:21:31.196091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 04:21:31.196229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 04:21:31.196239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "from functools import partial\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "import chex\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit, lax\n",
    "from jax import numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kpmurphy/github/label-shift/tta'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tta.utils.Dataset'>\n",
      "<class 'tta.datasets.MultipleDomainDataset'>\n",
      "<class 'tta.datasets.chexpert.MultipleDomainCheXpert'>\n"
     ]
    }
   ],
   "source": [
    "import tta\n",
    "from tta.utils import *\n",
    "print(Dataset)\n",
    "\n",
    "from tta.datasets import *\n",
    "print(MultipleDomainDataset)\n",
    "\n",
    "from tta.datasets.chexpert import *\n",
    "print(MultipleDomainCheXpert)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kpmurphy/github/label-shift/tta'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = '/home/kpmurphy/data/CheXpert'\n",
    "root = Path(root)\n",
    "labels = pd.read_csv(root / \"labels.csv\", index_col=\"image_id\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NO_FINDING</th>\n",
       "      <th>ENLARGED_CARDIOMEDIASTINUM</th>\n",
       "      <th>CARDIOMEGALY</th>\n",
       "      <th>AIRSPACE_OPACITY</th>\n",
       "      <th>LUNG_LESION</th>\n",
       "      <th>PULMONARY_EDEMA</th>\n",
       "      <th>CONSOLIDATION</th>\n",
       "      <th>PNEUMONIA</th>\n",
       "      <th>ATELECTASIS</th>\n",
       "      <th>...</th>\n",
       "      <th>EFFUSION</th>\n",
       "      <th>PLEURAL_OTHER</th>\n",
       "      <th>FRACTURE</th>\n",
       "      <th>SUPPORT_DEVICES</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>split</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE_AT_CXR</th>\n",
       "      <th>PRIMARY_RACE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CheXpert-v1.0/train/patient42720/study2/view1_frontal.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>patient42720</td>\n",
       "      <td>train</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheXpert-v1.0/train/patient42720/study7/view1_frontal.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>patient42720</td>\n",
       "      <td>train</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheXpert-v1.0/train/patient42720/study8/view1_frontal.jpg</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>patient42720</td>\n",
       "      <td>train</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheXpert-v1.0/train/patient42720/study6/view1_frontal.jpg</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>patient42720</td>\n",
       "      <td>train</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheXpert-v1.0/train/patient42720/study1/view1_frontal.jpg</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>patient42720</td>\n",
       "      <td>train</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Non-Latino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Unnamed: 0  NO_FINDING  \\\n",
       "image_id                                                                     \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...           0           3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...           1           3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...           2           3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...           3           3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...           4           3   \n",
       "\n",
       "                                                    ENLARGED_CARDIOMEDIASTINUM  \\\n",
       "image_id                                                                         \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...                           1   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...                           3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...                           3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...                           3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...                           3   \n",
       "\n",
       "                                                    CARDIOMEGALY  \\\n",
       "image_id                                                           \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...             3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...             0   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...             0   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...             3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...             3   \n",
       "\n",
       "                                                    AIRSPACE_OPACITY  \\\n",
       "image_id                                                               \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...                 3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...                 1   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...                 1   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...                 3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...                 3   \n",
       "\n",
       "                                                    LUNG_LESION  \\\n",
       "image_id                                                          \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...            3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...            3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...            3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...            3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...            3   \n",
       "\n",
       "                                                    PULMONARY_EDEMA  \\\n",
       "image_id                                                              \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...                1   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...                3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...                3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...                1   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...                1   \n",
       "\n",
       "                                                    CONSOLIDATION  PNEUMONIA  \\\n",
       "image_id                                                                       \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...              3          3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...              3          3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...              3          3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...              3          3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...              3          3   \n",
       "\n",
       "                                                    ATELECTASIS  ...  \\\n",
       "image_id                                                         ...   \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...            3  ...   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...            3  ...   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...            3  ...   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...            3  ...   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...            3  ...   \n",
       "\n",
       "                                                    EFFUSION  PLEURAL_OTHER  \\\n",
       "image_id                                                                      \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...         1              3   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...         1              3   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...         1              3   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...         1              3   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...         1              3   \n",
       "\n",
       "                                                    FRACTURE  SUPPORT_DEVICES  \\\n",
       "image_id                                                                        \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...         3                1   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...         3                1   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...         3                1   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...         3                1   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...         3                1   \n",
       "\n",
       "                                                      patient_id  split  \\\n",
       "image_id                                                                  \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...  patient42720  train   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...  patient42720  train   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...  patient42720  train   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...  patient42720  train   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...  patient42720  train   \n",
       "\n",
       "                                                   GENDER AGE_AT_CXR  \\\n",
       "image_id                                                               \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...   Male         58   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...   Male         58   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...   Male         58   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...   Male         58   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...   Male         58   \n",
       "\n",
       "                                                    PRIMARY_RACE  \\\n",
       "image_id                                                           \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...         White   \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...         White   \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...         White   \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...         White   \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...         White   \n",
       "\n",
       "                                                                  ETHNICITY  \n",
       "image_id                                                                     \n",
       "CheXpert-v1.0/train/patient42720/study2/view1_f...  Non-Hispanic/Non-Latino  \n",
       "CheXpert-v1.0/train/patient42720/study7/view1_f...  Non-Hispanic/Non-Latino  \n",
       "CheXpert-v1.0/train/patient42720/study8/view1_f...  Non-Hispanic/Non-Latino  \n",
       "CheXpert-v1.0/train/patient42720/study6/view1_f...  Non-Hispanic/Non-Latino  \n",
       "CheXpert-v1.0/train/patient42720/study1/view1_f...  Non-Hispanic/Non-Latino  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190499"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint the embeddings with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(root, max_rows=0):\n",
    "    labels = pd.read_csv(root / \"labels.csv\", index_col=\"image_id\")\n",
    "\n",
    "    # Extract subset of rows for which all labels are available\n",
    "    labels = labels.loc[labels[\"PNEUMONIA\"].isin({1, 3})]\n",
    "    labels = labels.loc[labels[\"EFFUSION\"].isin({1, 3})]\n",
    "    labels = labels.loc[labels[\"GENDER\"] != \"Unknown\"]\n",
    "\n",
    "    if max_rows == 0:\n",
    "        max_rows = len(labels)\n",
    "\n",
    "    columns = [\"PNEUMONIA\", \"EFFUSION\", \"GENDER\"]\n",
    "    for t in columns:\n",
    "        code, uniques = pd.factorize(labels[t], sort=True)\n",
    "        print(t, code, uniques) #1->0, 3->1, female->0, male->1\n",
    "        labels[t] = code\n",
    "    \n",
    "    m = np.median(labels[\"AGE_AT_CXR\"])\n",
    "    print('median age ', m)\n",
    "    labels[\"AGE_QUANTIZED\"] = (labels[\"AGE_AT_CXR\"] > m)\n",
    "    columns.append(\"AGE_QUANTIZED\")\n",
    "\n",
    "    YZ = labels[columns].to_numpy()\n",
    "    YZ = YZ[:max_rows]\n",
    "    return YZ, labels, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNEUMONIA [1 1 1 ... 0 0 0] Int64Index([1, 3], dtype='int64')\n",
      "EFFUSION [0 0 0 ... 0 0 0] Int64Index([1, 3], dtype='int64')\n",
      "GENDER [1 1 1 ... 1 0 0] Index(['Female', 'Male'], dtype='object')\n",
      "median age  62.0\n",
      "(139907, 4)\n"
     ]
    }
   ],
   "source": [
    "YZ, labels, columns = extract_labels(root, max_rows=0)\n",
    "print(YZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(YZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(root, labels, max_rows=20):\n",
    "    datastore = np.load(root / \"embeddings.npz\")\n",
    "    if max_rows == 0:\n",
    "        max_rows = len(labels)\n",
    "    ndims = 1376\n",
    "    X = np.zeros((max_rows, ndims))\n",
    "    i = 0\n",
    "    for fname in labels.index:\n",
    "        x = datastore[fname]\n",
    "        i += 1\n",
    "        if i >= max_rows: break\n",
    "        X[i,:] = x\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139907, 1376)\n",
      "CPU times: user 15min 47s, sys: 4.41 s, total: 15min 51s\n",
      "Wall time: 15min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = extract_features(root, labels, max_rows=0)\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PNEUMONIA', 'EFFUSION', 'GENDER', 'AGE_QUANTIZED']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(root / 'data_matrix.npz', X=X, YZ=YZ, columns=columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-computed data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'YZ', 'columns']\n",
      "(139907, 1376)\n",
      "(139907, 4)\n",
      "['PNEUMONIA' 'EFFUSION' 'GENDER' 'AGE_QUANTIZED']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "root = '/home/kpmurphy/data/CheXpert'\n",
    "root = Path(root)\n",
    "\n",
    "data = np.load(root / 'data_matrix.npz', allow_pickle=True)\n",
    "print(data.files)\n",
    "print(data['X'].shape)\n",
    "print(data['YZ'].shape)\n",
    "print(data['columns'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a logistic regression model with sklean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111925, 27982]\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "ndx = np.where(data['columns'] == 'EFFUSION')[0][0]\n",
    "Y = np.array(data['YZ'][:,ndx], dtype=int)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "N_train  = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print([N_train, N_test])\n",
    "\n",
    "classifier = Pipeline([\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        #('poly', PolynomialFeatures(degree=2)), \n",
    "        ('logreg', LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#N = 100\n",
    "N  = N_train\n",
    "XX = X_train[:N]\n",
    "YY = Y_train[:N]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(XX, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214352083482239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "y_pred2 = classifier.predict(X_test)\n",
    "assert np.allclose(y_pred, y_pred2)\n",
    "\n",
    "y_true = Y_test\n",
    "acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 05:07:15.415404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 05:07:15.449361: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-09 05:07:16.178259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 05:07:16.178429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-09 05:07:16.178441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit\n",
    "#import jax.debug\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "import chex\n",
    "import typing\n",
    "\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "import distrax\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logprior_fn(params, sigma):\n",
    "    # log p(params)\n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n",
    "    return jnp.sum(distrax.Normal(0, sigma).log_prob(flat_params))\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def get_batch_train_ixs(key, num_train, batch_size):\n",
    "    # return indices of training set in a random order\n",
    "    steps_per_epoch = num_train // batch_size\n",
    "    batch_ixs = jax.random.permutation(key, num_train)\n",
    "    batch_ixs = batch_ixs[:steps_per_epoch * batch_size]\n",
    "    batch_ixs = batch_ixs.reshape(steps_per_epoch, batch_size)\n",
    "    return batch_ixs\n",
    "\n",
    "\n",
    "class LogRegNetwork(nn.Module):\n",
    "    nclasses: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        logits = nn.Dense(self.nclasses)(x)\n",
    "        return logits\n",
    "\n",
    "class MLPNetwork(nn.Module):\n",
    "  nfeatures_per_layer: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    nlayers = len(self.nfeatures_per_layer)\n",
    "    for i, feat in enumerate(self.nfeatures_per_layer):\n",
    "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "      if i != (nlayers - 1):\n",
    "        #x = nn.relu(x)\n",
    "        x = nn.gelu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetClassifier:\n",
    "    def __init__(self, network, key, nclasses, *,  l2reg=1e-5, standardize = True,\n",
    "                optimizer = 'adam+warmup', batch_size=128, max_iter=100, num_epochs=10, print_every=0):\n",
    "        # optimizer is one of {'adam+warmup'} or an optax object\n",
    "        self.nclasses = nclasses\n",
    "        self.network = network\n",
    "        self.standardize = standardize\n",
    "        self.max_iter = max_iter\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.l2reg = l2reg\n",
    "        self.print_every = print_every\n",
    "        self.params = None # must first call fit\n",
    "        self.key = key\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.params is None:\n",
    "            raise ValueError('need to call fit before predict')\n",
    "        if self.standardize:\n",
    "            X = X - self.mean\n",
    "            X = X / self.std\n",
    "        return jax.nn.softmax(self.network.apply(self.params, X))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.standardize:\n",
    "            self.mean = jnp.mean(X, axis=0)\n",
    "            self.std = jnp.std(X, axis=0) + 1e-5 \n",
    "            X = X - self.mean\n",
    "            X = X / self.std\n",
    "        if self.params is None:\n",
    "            nfeatures = X.shape[1]\n",
    "            x = jr.normal(self.key, (nfeatures,)) # single random input \n",
    "            self.params = self.network.init(self.key, x) \n",
    "        ntrain = X.shape[0]\n",
    "        if isinstance(self.optimizer, str) and (self.optimizer.lower() == \"adam+warmup\"):\n",
    "            total_steps = self.num_epochs*(ntrain//self.batch_size)  \n",
    "            warmup_cosine_decay_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "                init_value=1e-3, peak_value=1e-1, warmup_steps=int(total_steps*0.1),\n",
    "                decay_steps=total_steps, end_value=1e-3)\n",
    "            self.optimizer = optax.adam(learning_rate=warmup_cosine_decay_scheduler)\n",
    "            return self.fit_optax(self.key, X, y)\n",
    "        else:\n",
    "            #return self.fit_jaxopt(self.key, X, y)\n",
    "            return self.fit_optax(self.key, X, y)\n",
    "\n",
    "    def fit_optax(self, key, X, y):\n",
    "        # Based on https://github.com/google/flax/blob/main/examples/mnist/train.py\n",
    "        # https://github.com/google/flax/blob/main/examples/vae/train.py\n",
    "        sigma = np.sqrt(1/self.l2reg)\n",
    "        ntrain = X.shape[0]\n",
    " \n",
    "        @jax.jit\n",
    "        def train_step(key, state, Xb, yb):\n",
    "            # Computes gradients, loss and accuracy for a single batch.\n",
    "            # loss = -1/N [ (sum_n log p(yn|xn, theta)) + log p(theta) ]\n",
    "            def loss_fn(params):\n",
    "                logits = state.apply_fn({'params': params}, Xb)\n",
    "                loglik = jnp.mean(distrax.Categorical(logits).log_prob(yb))\n",
    "                logjoint = loglik + (1/ntrain)*logprior_fn(params, sigma)\n",
    "                return -logjoint\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "            return loss, state.apply_gradients(grads=grads)\n",
    "\n",
    "        def train_epoch(key, state):\n",
    "            key, sub_key = jr.split(key)\n",
    "            batch_ixs = get_batch_train_ixs(sub_key, ntrain, self.batch_size)\n",
    "            num_batches = len(batch_ixs)\n",
    "            key, sub_key = jr.split(key)\n",
    "            keys = jax.random.split(sub_key, num_batches)    \n",
    "            total_loss = 0\n",
    "            for key, batch_ix in zip(keys, batch_ixs):\n",
    "                X_batch, y_batch = X[batch_ix], y[batch_ix]\n",
    "                loss, state = train_step(key, state, X_batch, y_batch)\n",
    "                total_loss += loss\n",
    "            return total_loss.item(), state\n",
    "\n",
    "\n",
    "        # main loop\n",
    "        state = train_state.TrainState.create(\n",
    "            apply_fn=self.network.apply, params=self.params['params'], tx=self.optimizer)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            key, sub_key = jr.split(key)\n",
    "            train_loss, state = train_epoch(sub_key, state)\n",
    "            if (self.print_every > 0) and (epoch % self.print_every == 0):\n",
    "                print('epoch {:d}, train loss {:0.3f}'.format(epoch, train_loss))\n",
    "\n",
    "        self.params = {'params': state.params}\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make shifted datasets for each domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/kpmurphy/data/CheXpert\")\n",
    "dataset_y_column = \"EFFUSION\"\n",
    "dataset_z_column = \"GENDER\"\n",
    "dataset_use_embedding = True\n",
    "train_domains_set = [9]\n",
    "target_domain_count = 512\n",
    "\n",
    "import random\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "\n",
    "dataset = MultipleDomainCheXpert(root, generator, dataset_y_column, dataset_z_column, dataset_use_embedding, train_domains_set, target_domain_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
